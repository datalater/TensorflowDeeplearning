{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-01]** 모듈을 임포트 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-02]** Placeholder x를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-03]** Variable w를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.zeros([5, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-04]** 계산식 y를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-05]** Placeholder t를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-06]** 오차 함수 loss를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(y-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-07]** 트레이닝 알고리즘 train_step을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on AdamOptimizer in module tensorflow.python.training.adam object:\n",
      "\n",
      "class AdamOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      " |  Optimizer that implements the Adam algorithm.\n",
      " |  \n",
      " |  See [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n",
      " |  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdamOptimizer\n",
      " |      tensorflow.python.training.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
      " |      Construct a new Adam optimizer.\n",
      " |      \n",
      " |      Initialization:\n",
      " |      \n",
      " |      ```\n",
      " |      m_0 <- 0 (Initialize initial 1st moment vector)\n",
      " |      v_0 <- 0 (Initialize initial 2nd moment vector)\n",
      " |      t <- 0 (Initialize timestep)\n",
      " |      ```\n",
      " |      \n",
      " |      The update rule for `variable` with gradient `g` uses an optimization\n",
      " |      described at the end of section2 of the paper:\n",
      " |      \n",
      " |      ```\n",
      " |      t <- t + 1\n",
      " |      lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
      " |      \n",
      " |      m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n",
      " |      v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\n",
      " |      variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n",
      " |      ```\n",
      " |      \n",
      " |      The default value of 1e-8 for epsilon might not be a good default in\n",
      " |      general. For example, when training an Inception network on ImageNet a\n",
      " |      current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
      " |      formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
      " |      the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
      " |      hat\" in the paper.\n",
      " |      \n",
      " |      The sparse implementation of this algorithm (used when the gradient is an\n",
      " |      IndexedSlices object, typically because of `tf.gather` or an embedding\n",
      " |      lookup in the forward pass) does apply momentum to variable slices even if\n",
      " |      they were not used in the forward pass (meaning they have a gradient equal\n",
      " |      to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
      " |      accumulator. This means that the sparse behavior is equivalent to the dense\n",
      " |      behavior (in contrast to some momentum implementations which ignore momentum\n",
      " |      unless a variable slice was actually used).\n",
      " |      \n",
      " |      Args:\n",
      " |        learning_rate: A Tensor or a floating point value.  The learning rate.\n",
      " |        beta1: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 1st moment estimates.\n",
      " |        beta2: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 2nd moment estimates.\n",
      " |        epsilon: A small constant for numerical stability. This epsilon is\n",
      " |          \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |          Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
      " |        use_locking: If True use locks for update operations.\n",
      " |        name: Optional name for the operations created when applying gradients.\n",
      " |          Defaults to \"Adam\".\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      " |      applies gradients.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      " |          `compute_gradients()`.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        name: Optional name for the returned operation.  Default to the\n",
      " |          name passed to the `Optimizer` constructor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that applies the specified gradients. If `global_step`\n",
      " |        was not None, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        ValueError: If none of the variables have gradients.\n",
      " |  \n",
      " |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      " |      Compute gradients of `loss` for the variables in `var_list`.\n",
      " |      \n",
      " |      This is the first part of `minimize()`.  It returns a list\n",
      " |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      " |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      " |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      " |      given variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A Tensor containing the value to minimize.\n",
      " |        var_list: Optional list or tuple of `tf.Variable` to update to minimize\n",
      " |          `loss`.  Defaults to the list of variables collected in the graph\n",
      " |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of (gradient, variable) pairs. Variable is always present, but\n",
      " |        gradient can be `None`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      " |        ValueError: If some arguments are invalid.\n",
      " |  \n",
      " |  get_name(self)\n",
      " |  \n",
      " |  get_slot(self, var, name)\n",
      " |      Return a slot named `name` created for `var` by the Optimizer.\n",
      " |      \n",
      " |      Some `Optimizer` subclasses use additional variables.  For example\n",
      " |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      " |      gives access to these `Variable` objects if for some reason you need them.\n",
      " |      \n",
      " |      Use `get_slot_names()` to get the list of slot names created by the\n",
      " |      `Optimizer`.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      " |        name: A string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      " |  \n",
      " |  get_slot_names(self)\n",
      " |      Return a list of the names of slots created by the `Optimizer`.\n",
      " |      \n",
      " |      See `get_slot()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of strings.\n",
      " |  \n",
      " |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      " |      Add operations to minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply combines calls `compute_gradients()` and\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A `Tensor` containing the value to minimize.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        var_list: Optional list or tuple of `Variable` objects to update to\n",
      " |          minimize `loss`.  Defaults to the list of variables collected in\n",
      " |          the graph under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |        name: Optional name for the returned operation.\n",
      " |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      " |        was not `None`, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If some of the variables are not `Variable` objects.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  GATE_GRAPH = 2\n",
      " |  \n",
      " |  GATE_NONE = 0\n",
      " |  \n",
      " |  GATE_OP = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.AdamOptimizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-08]** 세션을 준비하고 Variable을 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-09]** 트레이닝 세트 데이터를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_t = np.array([5.2, 5.7, 8.6, 14.9, 18.2, 20.4, 25.5, 26.4, 22.8, 17.5, 11.1, 6.6])\n",
    "train_t = train_t.reshape([12, 1])\n",
    "\n",
    "train_x = np.zeros([12, 5])\n",
    "for row, month in enumerate(range(1, 13)):\n",
    "    for col, n in enumerate(range(0, 5)):\n",
    "        train_x[row][col] = month**n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-10]** 경사 하강법을 이용한 파라미터 최적화를 100000회 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000, Loss: 19.512671\n",
      "Step: 20000, Loss: 19.512671\n",
      "Step: 30000, Loss: 19.512671\n",
      "Step: 40000, Loss: 19.512671\n",
      "Step: 50000, Loss: 19.512671\n",
      "Step: 60000, Loss: 19.512671\n",
      "Step: 70000, Loss: 19.512671\n",
      "Step: 80000, Loss: 19.512671\n",
      "Step: 90000, Loss: 19.512671\n",
      "Step: 100000, Loss: 19.512671\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in range(100000):\n",
    "    i += 1\n",
    "    sess.run(train_step, feed_dict={x: train_x, t: train_t})\n",
    "    if i % 10000 == 0:\n",
    "        loss_val = sess.run(loss, feed_dict={x: train_x, t: train_t})\n",
    "        print('Step: %d, Loss: %f' % (i, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-11]** 다시 100000회 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 110000, Loss: 19.973928\n",
      "Step: 120000, Loss: 19.552513\n",
      "Step: 130000, Loss: 18.860426\n",
      "Step: 140000, Loss: 18.366467\n",
      "Step: 150000, Loss: 17.948450\n",
      "Step: 160000, Loss: 18.789228\n",
      "Step: 170000, Loss: 17.088497\n",
      "Step: 180000, Loss: 16.720734\n",
      "Step: 190000, Loss: 16.423943\n",
      "Step: 200000, Loss: 19.512671\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100000):\n",
    "    i += 1\n",
    "    sess.run(train_step, feed_dict={x: train_x, t: train_t})\n",
    "    if i % 10000 ==0 :\n",
    "        loss_val = sess.run(loss, feed_dict={x: train_x, t: train_t})\n",
    "        print('Step: %d, Loss: %f' % (i, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-12]** 트레이닝 후 파라미터 값을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.3395257 ]\n",
      " [-4.2865386 ]\n",
      " [ 2.58276868]\n",
      " [-0.28977084]\n",
      " [ 0.00858687]]\n"
     ]
    }
   ],
   "source": [
    "w_val = sess.run(w)\n",
    "print (w_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-13]** 트레이닝 후 파라미터를 이용해 예측기온을 계산하는 함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    result = 0.0\n",
    "    for n in range(0, 5):\n",
    "        result += w_val[n][0] * x **n\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[LSE-14]** 예측기온 그래프를 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fecec2bdd8>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fecebdac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "subplot = fig.add_subplot(1,1,1)\n",
    "subplot.set_xlim(1,12)\n",
    "subplot.scatter(range(1,13), train_t)\n",
    "linex = np.linspace(1,12,100)\n",
    "liney = predict(linex)\n",
    "subplot.plot(linex, liney)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
