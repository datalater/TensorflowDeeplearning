{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ⓒ JMC 2017\n",
    "\n",
    "**SOURCE**  \n",
    "\\- 텐서플로로 시작하는 딥러닝, 나카이 에츠지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. 반복숙달용 Summary Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-1. 데이터를 모델링하는 3단계\n",
    "\n",
    "1. 주어진 데이터를 기반으로 미지의 데이터를 예측하는 '**모델 방정식**'을 생각한다.\n",
    "2. 모델 방정식에 포함된 파라미터의 좋고 나쁨을 판단하는 '**오차 함수**'를 준비한다.\n",
    "3. 오차 함수를 최소화하는 '**최적의 파라미터값**'을 결정한다.\n",
    "\n",
    "### Q1-2. 모델의 뜻\n",
    "\n",
    "+ 데이터를 설명하는 방정식\n",
    "\n",
    "### Q1-3. 머신러닝에서 컴퓨터가 하는 역할 (=텐서플로의 주요 업무)\n",
    "\n",
    "+ **파라미터 최적화** : 오차 함수를 최소화하는 계산을 정해진 알고리즘을 이용해 자동으로 계산하는 것이 머신러닝에서 컴퓨터가 하는 역할이며, 텐서플로의 주요 업무다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-4. 분류 문제에서 모델링하는 3단계\n",
    "\n",
    "1. 두 class의 경계를 나타내는 '**직선 형태의 모델 방정식(linear classifier)**'을 생각한다.\n",
    "> **Note**: 단, linear classifier를 sigmoid 함수에 대입해서 두 '**class에 속할 확률값**'을 도출한다.\n",
    "2. linear classifier에 포함된 파라미터의 좋고 나쁨을 판단하는 '**오차 함수**'를 준비한다.\n",
    "3. 오차 함수를 최소화하는 '**최적의 파라미터값**'을 결정한다.\n",
    "\n",
    "> **Note:** `linear classifier`란 분류 문제에서 사용하는 직선 형태의 모델을 뜻한다. || linear classifier 방정식 : $f(x_1, x_2) = w_0 + w_1x_1 + w_2x_2 = 0$ || linear classifier로 계산된 score 값을 0부터 1까지의 값을 가지는 함수에 대입하면 `각 class에 속할 확률값`인 $P(x_1, x_2)$를 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-5. 신경망이 데이터 모델링 3단계에서 가지는 의미\n",
    "\n",
    "+ 데이터 모델링 1단계에서 데이터를 설명하는 모델은 사람이 생각해내야 한다.\n",
    "+ 데이터의 차원이 아주 높다면 사람의 뇌로는 상상조차 할 수 없다.\n",
    "+ 그런데 '**신경망**'은 '**하나의 단순한 수식이 아니라 여러 개의 수식을 조합한 함수**'를 사용해서 다양한 데이터에 대응할 수 있는 모델이라는 데에 의미가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-6. 파라미터를 최적화하는 방법\n",
    "\n",
    "먼저 오차 함수는 파라미터에 대한 함수라는 것을 알아야 한다.\n",
    "파라미터가 포함된 모델의 방정식이 오차 함수에 들어가 있다.\n",
    "즉, **파라미터 값이 변경되면 오차 함수의 값도 변화하므로, 오차 함수는 파라미터에 대한 함수라고 볼 수 있다**.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (1.1) \\ L = \\frac{1}{2}\\Sigma(y_i - t_i)^{2} $\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (1.2) \\ y_i = w_0 + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 = \\Sigma w_ix_i$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (1.3) \\ L(w_0, w_1, w_2, w_3, w_4) = \\frac{1}{2}\\Sigma(\\Sigma w_ix_i - t_i)^{2}$\n",
    "\n",
    "**오차 함수를 다변수 공간에 표현했을 때 함수값이 최소로 되는 최저점을 찾아야 한다**.\n",
    "이 최저점은 '**기울기 벡터가 0이 되는 곳**'을 뜻한다.\n",
    "기울기 벡터는 오차 함수를 각 parameter별로 편미분하면 구할 수 있다.\n",
    "따라서 오차 함수를 편미분해서 기울기 벡터를 구하고 기울기 벡터가 0이 되는 곳을 찾으면, 오차 함수의 최소값을 구할 수 있게 된다.\n",
    "\n",
    "**기울기 벡터의 크기가 0이 되는 곳까지 현재 위치의 기울기 벡터와 반대 방향으로 반복해서 내려간다**.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (1.8) \\ W^{new} = W - \\alpha \\triangledown L(W)$\n",
    "+ $W$ : $(w_0, w_1, w_2, w_3, w_4)$의 현재 위치(=좌표)\n",
    "+ $- \\triangledown L(W)$ : 현재 위치 $W$에서의 기울기 벡터와 반대방향으로 간다\n",
    "+ $- \\alpha \\triangledown L(W)$ : 한 번 반대 방향으로 갈 때 $\\alpha$만큼 이동한다\n",
    "+ $W^{new}$ : 현재 위치에서 현재 위치에서의 기울기 벡터만큼 반대 방향으로 이동한 곳이 새로운 위치이다 (= 파라미터 갱신)\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (1.9) \\ \\triangledown L(W) = \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_0}(W) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w_4}(W)\n",
    "\\end{pmatrix}$\n",
    "\n",
    "이때 식 (1.8)에서 파라미터를 갱신할 때마다 그 점에서의 기울기 벡터 값을 식 (1.9)로 다시 계산한다는 점에 주의한다.\n",
    "이렇게 '**현재 파라미터의 값에 대해 기울기 벡터를 계산하고, 그 반대 방향으로 파라미터를 수정하는 알고리즘**'을 '**경사 하강법(gradient descent**)'이라고 한다.\n",
    "현실의 문제에서는 충분히 원점에 가까워졌을 때 계산을 중단하고, 그 시점의 값을 근사적인 최적해로 채택한다.\n",
    "파라미터를 갱신하는 계산은 당연히 컴퓨터를 이용해 자동화하며, 이 부분이 머신러닝이나 딥러닝에서 텐서플로의 역할에 해당한다.\n",
    "**training set의 양이 많은 경우 한 번에 모든 데이터를 이용해서 파라미터를 최적화하는 것이 아니라, 단계적으로 데이터를 투입해가면서 파라미터를 최적화해 가는 식의 테크닉도 필요하다**.\n",
    "\n",
    "그런데 **오차 함수의 그래프가 여러 곳에 local minima가 있다면** 기울기 벡터가 0이 되는 곳이 여러 군데 존재하게 된다.\n",
    "즉, 경사 하강법으로 global minimum에 도달하지 못할 수도 있다.\n",
    "이와 같은 문제에 대응하기 위해 **확률적 경사 하강법(SGD)이나 미니 배치(mini-batch) 같은 테크닉을 사용한다**.\n",
    "\n",
    "> **Note**: 2.3.4 미니배치와 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Summary Questions에 아직 등록되지 않은 Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 확률을 이용해서 모델을 만들면 생기는 이점\n",
    "\n",
    "+ 확률을 이용해서 모델을 만들면, 오차 함수로 자연스럽게 최우추정법(MLE)을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 로지스틱 회귀를 이용한 이항 분류기\n",
    "\n",
    "### 데이터 모델링 1단계 : 모델 방정식 세우기 (1)\n",
    "\n",
    "두 가지 검사 $x_1, x_2$를 토대로 바이러스 감염, 비감염 판정을 내리는 training data가 있다.\n",
    "training data를 시각화하면 $x_1, x_2$축을 가진 그래프에서 감염(o), 비감염(x)을 나타내는 데이터 포인트가 찍힌다.\n",
    "우리의 목적은 데이터의 분류이므로 데이터 포인트 (o)와 (x)를 분리하는 직선을 그을 수 있다.\n",
    "두 데이터의 경계를 나타내는 이 직선을 linear classifier라고 한다.\n",
    "linear classifier는 분류 문제에서 데이터의 경계를 나눠서, 어떤 데이터가 어느 클래스에 속하는지 설명해주므로 '모델'이 된다.\n",
    "모델은 다음과 같이 수식으로 나타낼 수 있다.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.1) \\ f(x_1, x_2) = w_0 + w_1x_1 + w_2x_2$\n",
    "\n",
    "### 데이터 모델링 1단계 : 모델 방정식 세우기 (2)\n",
    "\n",
    "우리의 목표는 새로운 검사 결과 $(x_1, x_2)$가 나왔을 때 이 환자가 실제로 감염되었는지 판정하는 것이다.\n",
    "$x_1, x_2$축을 가진 그래프에 새로운 환자의 데이터가 찍혔다고 해보자.\n",
    "이를 단순하게 감염(o) 또는 비감염(x)으로 분류하는 것이 아니라 이 환자가 바이러스에 감염되었을 확률을 구하고자 한다.\n",
    "$f(x_1,x_2)$의 값은 $\\pm \\infty$를 향해 변화하므로 확률 값으로 적절하지 않다.\n",
    "$f(x_1,x_2)$의 범위를 확률의 범위 0~1 사이로 맞추기 위해 시그모이드 함수에 대입한다.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.2) \\ -\\infty < f(x_1,x_2) < +\\infty$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.3) \\ 0 < \\sigma (x) = \\frac{1}{1 + e^{-x}} < 1$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.4) \\ 0 < \\sigma (f(x_1, x_2)) < 1 $\n",
    "\n",
    "결국 우리의 모델은 다음과 같이 표현된다.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.5) \\ P(x_1, x_2) = \\sigma (f(x_1, x_2)) = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 모델링 2단계 : 오차 함수 준비 (1)\n",
    "\n",
    "\"확률을 이용해서 예측 모델을 만들면, 파라미터의 좋고 나쁨을 평가하는 오차 함수로 자연스럽게 최우추정법(MLE)를 사용할 수 있다는 이점이 있다.\"\n",
    "식 (2.5)에서 만든 모델에 포함된 파라미터의 좋고 나쁨을 어떻게 판단할 수 있을까?\n",
    "주어진 training data를 잘 맞히는지 알아보면 된다.\n",
    "엄밀히 말하자면, 수많은 parameter 조합 중에서 training data를 예측할 확률이 가장 높은 parameter를 선택하면 된다.\n",
    "이와 같이 주어진 데이터를 바르게 예측할 확률을 최대화하는 것을 최우추정법(Maximum Likelihood Estimation)이라고 한다.\n",
    "\n",
    "> **Note**: 모델 방정식에 포함된 parameter가 가질 수 있는 값은 무한 개이다. 즉, 가능한 모델의 개수 또한 무한 개이다. 최우추정법에서는 모델 여러 개 중에서 주어진 데이터를 예측할 확률이 가장 높은 모델을 선택하는 알고리즘이다.\n",
    "\n",
    "트레이닝 데이터 N개를 표현하면 식 (2.6)과 같다.\n",
    "$n$번째 데이터 $(x_{1_{n}}, x_{2_{n}})$의 바이러스 감염 여부를 $t_n \\in \\{0, 1\\} $이라고 하자. $n$번째 데이터를 바르게 예측할 확률을 $P_n$이라고 할 때 식 (2.7)과 식 (2.8)이 성립한다.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.6) \\ (x_{1_{1}}, x_{2_{1}}), (x_{1_{2}}, x_{2_{2}}), \\cdot \\cdot \\cdot, (x_{1_{N}}, x_{2_{N}})$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.7) \\ [ \\begin{array}{ll} t_n=1 : P_n = P(x_{1_{n}}, x_{2_{n}}) \\\\\n",
    "t_n=0 : P_n = 1 - P(x_{1_{n}}, x_{2_{n}})\n",
    "\\end{array}$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.8) \\ P_n = \\{ P(x_{1_{n}}, x_{2_{n}}) \\}^{t_n} \\{ 1 - P(x_{1_{n}}, x_{2_{n}}) \\}^{1-t_n}$\n",
    "\n",
    "식 (2.8)의 이해를 돕기 위해 예를 들어보자. \n",
    "3번째 데이터 $(x_{1_{3}}, x_{2_{3}})$의 $t_3 = 1$이고, 모델의 예측값이 0.8이라고 해보자.\n",
    "$P_3 = P(x_{1_{3}}, x_{2_{3}}) = 0.8$이 된다.\n",
    "실제 데이터의 값 $t_n=1$일 때는 모델의 예측값이 1에 가까울수록 좋다.\n",
    "4번째 데이터 $(x_{1_{4}}, x_{2_{4}})$의 $t_n = 0$이고, 모델의 예측값이 0.3이라고 해보자.\n",
    "$P_4 = 1-P(x_{1_{4}}, x_{2_{4}}) = 1-0.3 = 0.7$이 된다.\n",
    "실제 데이터의 값 $t_n=0$일 때는 모델의 예측값이 0에 가까울수록 좋다.\n",
    "예를 통해서 두 가지를 알 수 있다. \n",
    "첫째, training data와 일치하는 모델일수록 $P_n$ 값이 1에 가까워지게 된다.\n",
    "둘째, $P_n$ 값의 범위는 $0 \\leq P_n \\leq 1$이 된다.\n",
    "\n",
    "$n$번째 데이터가 아니라 $N$개 데이터 모두 정답일 확률 $P$를 계산해보자.\n",
    "식 (2.9)에 나와 있듯이, 각 데이터를 바르게 예측할 확률의 곱셈으로 계산할 수 있다.\n",
    "식 (2.10)이 우리가 만든 모델 (2.5)의 우도 함수이자 오차 함수이다.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.9) \\ P = P_1 \\times P_2 \\times \\cdots \\times P_N = \\Pi_{n=1}^{N}P_n$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.10) \\ P = \\Pi_{n=1}^{N}\\{ P(x_{1_{n}}, x_{2_{n}}) \\}^{t_n} \\{ 1 - P(x_{1_{n}}, x_{2_{n}}) \\}^{1-t_n}$\n",
    "\n",
    "$P_n$ 값의 범위가 $0 \\leq P_n \\leq 1$이므로 $P$의 범위 또한 $0 \\leq P \\leq 1$이 된다.\n",
    "\n",
    "#### 데이터 모델링 2단계 : 오차 함수 준비 (2)\n",
    "\n",
    "텐서플로로 계산할 경우 식 (2.10)과 같이 곱셈을 대량으로 포함하는 수식은 계산 효율이 좋지 않다.\n",
    "곱셉식을 덧셈으로 대체하려면 오차 함수에 $\\log$를 취하면 된다.\n",
    "따라서 $P$ 대신 $\\log P$로 나타낼 수 있다.\n",
    "$\\log$를 취하면 값의 범위가 아래 식처럼 변한다.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.11) \\ 0 \\leq P \\leq 1$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.12) \\ -\\infty \\leq \\log P \\leq 0$ \n",
    "\n",
    "그런데 일반적으로 오차 함수가 내뱉는 값은 오차가 되어야 하고, 우리는 오차 함수의 값을 최소화하도록 판을 짜야 한다.\n",
    "그래서 $\\log P$에 마이너스를 취한다.\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.13) \\ 0 \\leq -\\log P \\leq \\infty$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.14) \\ L = - \\log P$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.15) \\ L = - \\log \\Pi_{n=1}^{N}\\{ P(x_{1_{n}}, x_{2_{n}}) \\}^{t_n} \\{ 1 - P(x_{1_{n}}, x_{2_{n}}) \\}^{1-t_n}$\n",
    "\n",
    "$ \\ \\ \\ \\ \\ (2.16) \\ L = - \\Sigma_{n=1}^{N} [t_n \\log P(x_{1_{n}}, x_{2_{n}}) + (1 - t_n) \\log \\{ 1 - P(x_{1_{n}}, x_{2_{n}}) \\}]$\n",
    "\n",
    "이렇게 해서 오차 함수가 식 (2.16)으로 완성되었다.\n",
    "\n",
    "> **Note**: 로그함수는 단조 증가하는 함수이므로 $P$를 최대로 하는 것과 $-\\log P$를 최소로 하는 것은 동치가 된다. 오차 함수를 통해 우리가 알고자 하는 것은 \"parameter $w$의 값이 어떻게 달라져야 오차가 적어지는지\"이다. 이를 구하는 방법은 parameter $w$를 하나 하나 변경하면서 오차를 관찰하는 방법과 오차 함수가 최저값이 될 때 parameter $w$는 어떤 값을 가지는지 관찰하는 방법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 02 텐서플로를 이용한 최우추정 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-01]** 모듈을 임포트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy.random import multivariate_normal, permutation\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-02]** 트레이닝 세트 데이터를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20160512)\n",
    "\n",
    "n0, mu0, variance0 = 20, [10, 11], 20\n",
    "data0 = multivariate_normal(mu0, np.eye(2)*variance0, n0)\n",
    "df0 = DataFrame(data0, columns=['x1', 'x2'])\n",
    "df0['t'] = 0\n",
    "\n",
    "n1, mu1, variance1 = 15, [18, 20], 22\n",
    "data1 = multivariate_normal(mu1, np.eye(2)*variance1, n1)\n",
    "df1 = DataFrame(data1, columns=['x1', 'x2'])\n",
    "df1['t'] = 1\n",
    "\n",
    "df = pd.concat([df0, df1], ignore_index=True)\n",
    "train_set = df.reindex(permutation(df.index)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1 : 난수의 seed를 지정하는 것으로 여기서 지정한 값에 따라 이후에 생성될 난수의 패턴이 결정된다. \n",
    "    > **Note**: 시드를 명시적으로 지정하면 매번 동일한 데이터가 생성되므로 난수를 이용하더라도 동일한 데이터로 반복 테스트할 수 있게 된다.\n",
    "+ 3~6 : $t=0$(비감염)인 데이터를 난수로 생성\n",
    "+ 8~11 : $t=1$(감염)인 데이터를 난수로 생성\n",
    "+ 13~14 : 모든 데이터를 하나로 모은 후 행의 순번을 무작위로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-04]** (x1, x2)와 t를 각각 모은 것을 NumPy의 array 오브젝트로 추출해둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_set[['x1','x2']].as_matrix()\n",
    "train_t = train_set['t'].as_matrix().reshape([len(train_set),1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 텐서플로로 계산할 때는 각 종데이터를 다차원 배열, 즉 행렬 형태로 표현할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-05]** 트레이닝 세트 데이터에 대해 t=1(감염)일 확률을 구하는 계산식 p를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "w = tf.Variable(tf.zeros([2,1]))\n",
    "w0 = tf.Variable(tf.zeros([1]))\n",
    "f = tf.matmul(x, w) + w0\n",
    "p = tf.sigmoid(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1 : x는 35 by 2 행렬이지만, Placeholder에는 임의의 개수의 데이터가 들어갈 수 있도록 [None, 2]라는 크기를 지정한다.\n",
    "+ 4 : `tf.matmul(x, w)`는 다차원 배열이고, `w0`은 1차원 리스트이지만, 다차원 리스트에 하나의 요소로 된 값을 더한 경우 리스트의 각 요소에 동일한 값이 더해진다. 이를 브로드캐스팅 규칙이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-06]** 오차 함수 loss와 트레이닝 알고리즘 train_step을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = tf.placeholder(tf.float32, [None, 1])\n",
    "loss = -tf.reduce_sum(t*tf.log(p) + (1-t)*tf.log(1-p))\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2 : $log$를 취한 loss 함수를 나타낸 식이다. `tf.reduce_sum`은 함수식에서 $\\Sigma_{n=1}^{N}$에 해당한다.\n",
    "    + loss 함수식 : $ \\ \\ \\ \\ \\ (2.16) \\ L = - \\Sigma_{n=1}^{N} [t_n \\log P(x_{1_{n}}, x_{2_{n}}) + (1 - t_n) \\log \\{ 1 - P(x_{1_{n}}, x_{2_{n}}) \\}]$\n",
    "    + `tf.reduce_sum`은 행렬 혹은 다차원 리스트의 모든 요소를 합산하는 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-07]** 정답률 accuracy를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1 : $P_n \\geq 0.5$이면 $t=1$, 그렇지 않으면 $t=0$이라고 단순하게 예측하는 방식을 이용해서 정답률을 구한다. $P_n - 0.5$와 $t_n - 0.5$의 부호를 비교하면 된다.\n",
    "+ 2 : 1행에서 Bool 값으로 변환된 1과 0으로 이루어진 `correct_prediction` 벡터를 float32 타입으로 바꾸고 평균값을 계산한다.\n",
    "    + `tf.reduce_mean`은 다차원 리스트의 각 성분의 평균값을 계산하는 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-08]** 세션을 준비하고 Variable을 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 파라미터를 최적화하기 위해 세션을 준비하고 Variable 값을 초기화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-09]** 경사 하강법에 의한 파라미터 최적화를 20000회 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000, Loss: 10.772636, Accuracy: 0.914286\n",
      "Step: 4000, Loss: 8.197757, Accuracy: 0.971429\n",
      "Step: 6000, Loss: 6.576120, Accuracy: 0.971429\n",
      "Step: 8000, Loss: 5.511970, Accuracy: 0.942857\n",
      "Step: 10000, Loss: 4.798052, Accuracy: 0.942857\n",
      "Step: 12000, Loss: 4.314208, Accuracy: 0.942857\n",
      "Step: 14000, Loss: 3.986283, Accuracy: 0.942857\n",
      "Step: 16000, Loss: 3.766516, Accuracy: 0.942857\n",
      "Step: 18000, Loss: 3.623070, Accuracy: 0.942857\n",
      "Step: 20000, Loss: 3.534100, Accuracy: 0.942857\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in range(20000):\n",
    "    i += 1\n",
    "    sess.run(train_step, feed_dict={x: train_x, t: train_t})\n",
    "    if i % 2000 == 0:\n",
    "        loss_val, acc_val = sess.run(\n",
    "        [loss, accuracy], feed_dict={x: train_x, t: train_t})\n",
    "        print ('Step: %d, Loss: %f, Accuracy: %f'\n",
    "              % (i, loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 6~7 : 해당 시점에서의 Variable의 값을 이용해 loss와 accuracy의 값을 계산해서 각각 loss_val과 acc_val에 저장한다.\n",
    "    + 세션 내에서 계산값을 평가할 때는 이 예에 있는 [loss, accuracy]와 같이 리스트 형식으로 복수의 변수를 지정함으로써 여러 값을 동시에 얻을 수 있다.\n",
    "    + sess.run의 fetch argument에는 계산하고 싶은 값을 넣는다.\n",
    "+ 실행 결과 : 애초에 이 트레이닝 세트 데이터를 직선으로 완전하게 분류할 수는 없으므로 정답률(Accuracy)이 100%가 되는 일은 원칙적으로 불가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-10]** 이 시점의 파라미터 값을 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-17.1252 0.615048 0.538803\n"
     ]
    }
   ],
   "source": [
    "w0_val, w_val = sess.run([w0, w])\n",
    "w0_val, w1_val, w2_val = w0_val[0], w_val[0][0], w_val[1][0]\n",
    "print w0_val, w1_val, w2_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MLE-11]** 추출한 파라미터 값을 이용해 결과를 그래프로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8ec66dcd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAFpCAYAAAC8p8I3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVUYDt1JKVRkN3EopVWX+f1H12xiovNQGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff8ec6fe550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set0 = train_set[train_set['t']==0]\n",
    "train_set1 = train_set[train_set['t']==1]\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "subplot = fig.add_subplot(1,1,1)\n",
    "subplot.set_ylim([0, 30])\n",
    "subplot.set_xlim([0, 30])\n",
    "subplot.scatter(train_set1.x1, train_set1.x2, marker='x')\n",
    "subplot.scatter(train_set0.x1, train_set0.x2, marker='o')\n",
    "\n",
    "linex = np.linspace(0,30,10)\n",
    "liney = - (w1_val*linex/w2_val + w0_val/w2_val)\n",
    "subplot.plot(linex, liney)\n",
    "\n",
    "field = [[(1 / (1 + np.exp(-(w0_val + w1_val*x1 + w2_val*x2))))\n",
    "         for x1 in np.linspace(0,30,100)]\n",
    "        for x2 in np.linspace(0,30,100)]\n",
    "subplot.imshow(field, origin='lower', extent=(0,30,0,30),\n",
    "              cmap=plt.cm.gray_r, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1~2 : 트레이닝 세트 데이터 중에서 $t=0$와 $t=1$인 데이터를 개별로 추출한다.\n",
    "+ 4~9 : 서로 다른 기호(x, o)를 이용해서 산포도를 그린다.\n",
    "+ 11~13 : 앞서 구한 파라미터값을 이용해서 직선 $f(x_1, x_2)$을 그린다.\n",
    "+ 15~19 : 확률 $P(x_1,x_2)$의 변화를 색의 농담으로 표히나다.\n",
    "    + $(x_1,x_2)$ 평면을 100 by 100의 셀로 분할하고, 각각의 셀에 대해 $P(x_1,x_2)$의 값을 2차원 리스트인 field에 저장한 후 이를 색의 농담으로 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 테스트 세트를 이용한 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[CAF-01]** 모듈을 임포트하고 난수의 시드를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(20160531)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[CAF-02]** 트레이닝 세트 데이터를 준비하고 20%의 데이터를 테스트 세트로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0, mu0, variance0 = 800, [10, 11], 20\n",
    "data0 = multivariate_normal(mu0, np.eye(2)*variance0, n0)\n",
    "df0 = DataFrame(data0, columns=['x','y'])\n",
    "df0['t'] = 0\n",
    "\n",
    "n1, mu1, variance1 = 600, [18, 20], 22\n",
    "data1 = multivariate_normal(mu1, np.eye(2)*variance1, n1)\n",
    "df1 = DataFrame(data1, columns=['x','y'])\n",
    "df1['t'] = 1\n",
    "\n",
    "df = pd.concat([df0,df1], ignore_index=True)\n",
    "df = df.reindex(permutation(df.index)).reset_index(drop=True)\n",
    "\n",
    "num_data = int(len(df)*0.8)\n",
    "train_set = df[:num_data]\n",
    "test_set = df[num_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 난수로 데이터를 생성한 후 80%는 training set, 20%는 test set으로 나눈다.\n",
    "+ test set의 양이 너무 적지 않도로 전체적으로 이전보다 40배의 데이터를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 중간 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 소프트맥스 함수와 다항 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
